# Algoritmo Perceptron para PLN - Explica√ß√£o

## Vis√£o Geral
Esta implementa√ß√£o demonstra um **classificador perceptron** para Processamento de Linguagem Natural (PLN), especificamente para an√°lise de sentimentos bin√°ria de avalia√ß√µes de filmes.

## Como o Perceptron Funciona

### 1. **Arquitetura**
```
Camada de Entrada (Features de Texto) ‚Üí Soma Ponderada ‚Üí Fun√ß√£o de Ativa√ß√£o ‚Üí Sa√≠da (0 ou 1)
```

O perceptron √© a forma mais simples de rede neural com:
- **Entrada**: Vetor de caracter√≠sticas representando texto
- **Pesos**: Par√¢metros aprendidos (um por caracter√≠stica)
- **Vi√©s**: Par√¢metro √∫nico de deslocamento aprendido
- **Ativa√ß√£o**: Fun√ß√£o degrau (0 se soma < 0, caso contr√°rio 1)

### 2. **Pipeline de Pr√©-processamento de Texto**

#### Tokeniza√ß√£o
```
"Este filme foi √≥timo!" ‚Üí ["este", "filme", "foi", "√≥timo"]
```

#### Constru√ß√£o do Vocabul√°rio
- Extrair todas as palavras dos dados de treinamento
- Manter apenas as N palavras mais frequentes (ex: 1000)
- Criar mapeamento palavra-para-√≠ndice

#### Vetoriza√ß√£o (Bag-of-Words)
```
Texto: "√≥timo filme √≥timo atua√ß√£o"
Vocabul√°rio: {√≥timo: 0, filme: 1, atua√ß√£o: 2, ruim: 3}
Vetor: [2, 1, 1, 0]  # Contagem de cada palavra
```

### 3. **Algoritmo de Treinamento**

O perceptron usa a **Regra de Aprendizado do Perceptron**:

```
Para cada exemplo de treinamento (x, y):
  1. Fazer previs√£o: ≈∑ = sign(w¬∑x + b)
  2. Se ≈∑ ‚â† y (previs√£o est√° errada):
     - Atualizar: w = w + Œ∑(y - ≈∑)x
     - Atualizar: b = b + Œ∑(y - ≈∑)
```

Onde:
- `w` = vetor de pesos
- `x` = vetor de caracter√≠sticas de entrada
- `b` = vi√©s
- `Œ∑` = taxa de aprendizado
- `y` = r√≥tulo verdadeiro (0 ou 1)
- `≈∑` = r√≥tulo previsto

### 4. **Componentes Principais**

#### Extra√ß√£o de Features
```python
def texto_para_vetor(self, texto):
    vetor = np.zeros(len(self.vocabulario))
    tokens = self.tokenizar(texto)
    for token in tokens:
        if token in self.vocabulario:
            idx = self.vocabulario[token]
            vetor[idx] += 1  # Contar frequ√™ncia
    return vetor
```

#### Previs√£o
```python
def prever(self, x):
    ativacao = np.dot(x, self.pesos) + self.vies
    return 1 if ativacao >= 0 else 0
```

#### Atualiza√ß√£o de Pesos
```python
if previsao != y[i]:
    atualizacao = self.taxa_aprendizado * (y[i] - previsao)
    self.pesos += atualizacao * X[i]
    self.vies += atualizacao
```

## Exemplo Passo a Passo

### Dados de Treinamento
```
Avalia√ß√µes positivas (r√≥tulo=1):
- "Este filme foi absolutamente maravilhoso"
- "√ìtima atua√ß√£o e enredo brilhante"

Avalia√ß√µes negativas (r√≥tulo=0):
- "Este filme foi terr√≠vel e entediante"
- "Filme horr√≠vel com atua√ß√£o ruim"
```

### Processo de Aprendizado

**Estado inicial**: Todos os pesos = 0, vi√©s = 0

**Itera√ß√£o 1**:
- Entrada: "maravilhoso filme" ‚Üí vetor: [0, 1, 0, 1, 0]
- Previs√£o: 0 (errado, deveria ser 1)
- Atualizar pesos para "maravilhoso" e "filme" positivamente

**Itera√ß√£o 2**:
- Entrada: "terr√≠vel filme" ‚Üí vetor: [1, 1, 0, 0, 0]
- Previs√£o: 1 (errado, deveria ser 0)
- Atualizar pesos para "terr√≠vel" e "filme" negativamente

**Ap√≥s converg√™ncia**:
- Palavras positivas (maravilhoso, √≥timo) t√™m pesos positivos
- Palavras negativas (terr√≠vel, horr√≠vel) t√™m pesos negativos

## Formula√ß√£o Matem√°tica

### Fronteira de Decis√£o
```
f(x) = sign(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b)
```

Onde:
- x‚ÇÅ, x‚ÇÇ, ..., x‚Çô s√£o frequ√™ncias de palavras
- w‚ÇÅ, w‚ÇÇ, ..., w‚Çô s√£o pesos aprendidos
- b √© o termo de vi√©s

### Teorema de Converg√™ncia
O perceptron tem garantia de converg√™ncia se:
1. Os dados s√£o **linearmente separ√°veis**
2. Taxa de aprendizado > 0

## Vantagens

1. **Simples**: F√°cil de entender e implementar
2. **R√°pido**: Tempo de treinamento O(n) por itera√ß√£o
3. **Interpret√°vel**: Pesos mostram import√¢ncia das palavras
4. **Aprendizado Online**: Pode atualizar com novos dados incrementalmente

## Limita√ß√µes

1. **Apenas Linear**: N√£o pode aprender padr√µes n√£o-lineares
2. **Sem Sa√≠da Probabil√≠stica**: Apenas classifica√ß√£o bin√°ria
3. **Sens√≠vel a Outliers**: Pontos mal classificados influenciam fortemente o aprendizado
4. **Requer Dados Linearmente Separ√°veis**: N√£o converge caso contr√°rio

## Melhorias & Extens√µes

### 1. **Melhor Extra√ß√£o de Features**
- TF-IDF em vez de contagens brutas
- N-gramas (bigramas, trigramas)
- Embeddings de palavras (Word2Vec, GloVe)

### 2. **Classifica√ß√£o Multi-classe**
- Abordagem um-contra-todos
- M√∫ltiplos perceptrons

### 3. **Algoritmos Avan√ßados**
- Perceptron multi-camadas (MLP)
- M√°quinas de Vetores de Suporte (SVM)
- Regress√£o Log√≠stica (adiciona ativa√ß√£o sigmoide)

## Exemplo de Uso

```python
# Criar perceptron
perceptron = PerceptronTexto(taxa_aprendizado=0.1, epocas=100)

# Treinar em avalia√ß√µes de filmes
textos_treino = ["√ìtimo filme!", "Filme terr√≠vel", ...]
rotulos_treino = [1, 0, ...]
perceptron.treinar(textos_treino, rotulos_treino)

# Fazer previs√µes
texto_teste = "Este foi um filme incr√≠vel"
previsao = perceptron.prever(perceptron.texto_para_vetor(texto_teste))
# previsao = 1 (positivo)
```

## M√©tricas de Desempenho

Da sa√≠da do exemplo:
- **Acur√°cia de Treinamento**: 100% (convergiu em 3 √©pocas)
- **Acur√°cia de Teste**: 100%
- **Tamanho do Vocabul√°rio**: 65 palavras
- **Features Mais Importantes**: "decepcionante" (negativo), "incr√≠vel" (positivo)

## Quando Usar Perceptron para PLN

**Bom para**:
- Classifica√ß√£o de texto bin√°ria
- Modelos baseline r√°pidos
- Prop√≥sitos educacionais
- Modelos simples e interpret√°veis

**Melhores alternativas**:
- Regress√£o Log√≠stica (sa√≠das probabil√≠sticas)
- Naive Bayes (funciona bem para texto)
- Redes Neurais (padr√µes complexos)
- Transformers (estado da arte, ex: BERT)

## Conceitos-Chave para Lembrar

### üìä F√≥rmula Principal
```
Sa√≠da = sign(Œ£(peso_i √ó feature_i) + vi√©s)
```

### üîÑ Regra de Atualiza√ß√£o
```
peso_novo = peso_antigo + taxa √ó erro √ó entrada
```

### üéØ Condi√ß√£o de Converg√™ncia
- Dados devem ser **linearmente separ√°veis**
- Existe um hiperplano que separa as classes

### üí° Interpretabilidade
- **Pesos positivos** ‚Üí palavras que indicam classe positiva
- **Pesos negativos** ‚Üí palavras que indicam classe negativa
- **Magnitude do peso** ‚Üí import√¢ncia da palavra

## Fluxo de Trabalho Completo

```
1. COLETA DE DADOS
   ‚Üì
2. PR√â-PROCESSAMENTO (tokeniza√ß√£o, limpeza)
   ‚Üì
3. CONSTRU√á√ÉO DE VOCABUL√ÅRIO
   ‚Üì
4. VETORIZA√á√ÉO (bag-of-words)
   ‚Üì
5. TREINAMENTO (atualiza√ß√£o iterativa de pesos)
   ‚Üì
6. AVALIA√á√ÉO (m√©tricas de desempenho)
   ‚Üì
7. PREVIS√ÉO (novos textos)
```

## Compara√ß√£o com Outros Algoritmos

| Algoritmo | Complexidade | Interpretabilidade | Desempenho |
|-----------|--------------|-------------------|------------|
| Perceptron | Baixa | Alta | Bom (dados lineares) |
| Naive Bayes | Baixa | Alta | Bom (texto) |
| SVM | M√©dia | M√©dia | Muito Bom |
| Redes Neurais | Alta | Baixa | Excelente |
| BERT/Transformers | Muito Alta | Muito Baixa | Estado da Arte |

## Dicas Pr√°ticas

### ‚úÖ Fa√ßa
- Use normaliza√ß√£o de texto (lowercase, remo√ß√£o de pontua√ß√£o)
- Remova stopwords se apropriado
- Experimente diferentes taxas de aprendizado
- Valide com dados separados de teste
- Analise os pesos para interpretar o modelo

### ‚ùå Evite
- Usar em dados com padr√µes n√£o-lineares complexos
- Ignorar desbalanceamento de classes
- Treinar sem valida√ß√£o
- Usar vocabul√°rio muito grande sem sele√ß√£o de features
- Esperar resultados perfeitos em problemas complexos

## Aplica√ß√µes no Mundo Real

1. **An√°lise de Sentimentos**: Classificar reviews, tweets, coment√°rios
2. **Detec√ß√£o de Spam**: Filtrar emails indesejados
3. **Categoriza√ß√£o de Documentos**: Organizar artigos por t√≥pico
4. **Modera√ß√£o de Conte√∫do**: Detectar conte√∫do inapropriado
5. **Sistemas de Recomenda√ß√£o**: Classificar prefer√™ncias de usu√°rios

## Recursos Adicionais

### üìö Para Aprender Mais
- "Pattern Recognition and Machine Learning" - Bishop
- "Introduction to Machine Learning" - Alpaydin
- Curso de Machine Learning - Andrew Ng (Coursera)
- Documenta√ß√£o scikit-learn

### üõ†Ô∏è Ferramentas e Bibliotecas
- **scikit-learn**: Implementa√ß√£o pronta de perceptron
- **NLTK/spaCy**: Pr√©-processamento de texto
- **pandas**: Manipula√ß√£o de dados
- **matplotlib**: Visualiza√ß√£o de resultados

## Conclus√£o

O perceptron fornece uma base s√≥lida para entender como modelos de machine learning processam dados de texto. Embora o PLN moderno use abordagens mais sofisticadas, os conceitos centrais‚Äîextra√ß√£o de features, aprendizado de pesos e combina√ß√£o linear‚Äîpermanecem fundamentais para todas as arquiteturas de redes neurais.

**A simplicidade do perceptron √© sua maior for√ßa educacional**: ele desmistifica o aprendizado de m√°quina e mostra que, com a matem√°tica certa, podemos ensinar computadores a entender linguagem humana!

### üéì Pr√≥ximos Passos
1. Implemente varia√ß√µes (taxa de aprendizado adaptativa)
2. Experimente com diferentes features (TF-IDF, n-gramas)
3. Compare com outros algoritmos
4. Aplique em seus pr√≥prios dados de texto
5. Explore redes neurais multi-camadas (MLP)

**Lembre-se**: Todo grande modelo de IA come√ßou com fundamentos simples como o perceptron. Dominar o b√°sico √© essencial para compreender o avan√ßado! üöÄ
